{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC855 - Data analysis pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing spark and data\n",
    " - Spark version 2.2.0\n",
    " - Change /home/henrique/Downloads/spark to the path you downloaded and extracted spark\n",
    " - You may need to export hadoop bin to your path before running jupyter notebook \n",
    "   - `export PATH=$PATH:/Users/hadoop/hadoop/bin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import findspark \n",
    "import findspark\n",
    "\n",
    "# Initialize and provide path\n",
    "findspark.init(\"/home/henrique/Downloads/spark\")\n",
    "\n",
    "# Or use this alternative\n",
    "#findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "       .master(\"local\") \\\n",
    "       .appName(\"Linear Regression Model\") \\\n",
    "       .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "       .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deprecated - Using direct load from CSV to df\n",
    "#file = sc.textFile(\"games.csv\")\n",
    "#header = file.first()                           #extract header\n",
    "#data = file.filter(lambda row: row != header)   #filter out header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to load directly from the csv\n",
    "if this is working, the last frame is not needed anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sql = pyspark.sql.SQLContext(sc)\n",
    "\n",
    "df = (sql.read\n",
    "         .format(\"com.databricks.spark.csv\")  # Choose the bib to oad csv\n",
    "         .option(\"header\", \"true\")            # Use the first line as header\n",
    "         .option(\"inferSchema\", \"true\")       # Try to infer data type - if this is not set all the typer will be str\n",
    "         .load(\"games.csv\"))                  # File name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[gameId: bigint, gameDuration: int, seasonId: int, winner: int, firstBlood: int, firstTower: int, firstInhibitor: int, firstBaron: int, firstDragon: int, firstRiftHerald: int, t1_champ1id: int, t1_champ1_sum1: int, t1_champ1_sum2: int, t1_champ2id: int, t1_champ2_sum1: int, t1_champ2_sum2: int, t1_champ3id: int, t1_champ3_sum1: int, t1_champ3_sum2: int, t1_champ4id: int, t1_champ4_sum1: int, t1_champ4_sum2: int, t1_champ5id: int, t1_champ5_sum1: int, t1_champ5_sum2: int, t1_towerKills: int, t1_inhibitorKills: int, t1_baronKills: int, t1_dragonKills: int, t1_riftHeraldKills: int, t1_ban1: int, t1_ban2: int, t1_ban3: int, t1_ban4: int, t1_ban5: int, t2_champ1id: int, t2_champ1_sum1: int, t2_champ1_sum2: int, t2_champ2id: int, t2_champ2_sum1: int, t2_champ2_sum2: int, t2_champ3id: int, t2_champ3_sum1: int, t2_champ3_sum2: int, t2_champ4id: int, t2_champ4_sum1: int, t2_champ4_sum2: int, t2_champ5id: int, t2_champ5_sum1: int, t2_champ5_sum2: int, t2_towerKills: int, t2_inhibitorKills: int, t2_baronKills: int, t2_dragonKills: int, t2_riftHeraldKills: int, t2_ban1: int, t2_ban2: int, t2_ban3: int, t2_ban4: int, t2_ban5: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just checking if data is Ok\n",
    "will print the first three lines of file and the last two of then must be equal to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#file.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting each element of the data list and turning each line in a list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#file = file.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the header of the rdd and turning it into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map the RDD to a DF\n",
    "#from pyspark.sql import Row\n",
    "\n",
    "#df = file.map(lambda line: Row(gameId =                 line[0],\n",
    "#                                gameDuration =          line[1],\n",
    "#                                seasonId =              line[2],\n",
    "#                                winner =                line[3],\n",
    "#                                firstBlood =            line[4],\n",
    "#                                firstTower =            line[5],\n",
    "#                                firstInhibitor =        line[6],\n",
    "#                                firstBaron =            line[7],\n",
    "#                                firstDragon =           line[8],\n",
    "#                                firstRiftHerald =       line[9],\n",
    "#                                t1_champ1id =           line[10],\n",
    "#                                t1_champ1_sum1 =        line[11],\n",
    "#                                t1_champ1_sum2 =        line[12],\n",
    "#                                t1_champ2id =           line[13],\n",
    "#                                t1_champ2_sum1 =        line[14],\n",
    "#                                t1_champ2_sum2 =        line[15],\n",
    "#                                t1_champ3id =           line[16],\n",
    "#                                t1_champ3_sum1 =        line[17],\n",
    "#                                t1_champ3_sum2 =        line[18],\n",
    "#                                t1_champ4id =           line[19],\n",
    "#                                t1_champ4_sum1 =        line[20],\n",
    "#                                t1_champ4_sum2 =        line[21],\n",
    "#                                t1_champ5id =           line[22],\n",
    "#                                t1_champ5_sum1 =        line[23],\n",
    "#                                t1_champ5_sum2 =        line[24],\n",
    "#                                t1_towerKills =         line[25],\n",
    "#                                t1_inhibitorKills =     line[26],\n",
    "#                                t1_baronKills =         line[27],\n",
    "#                                t1_dragonKills =        line[28],\n",
    "#                                t1_riftHeraldKills =    line[29],\n",
    "#                                t1_ban1 =               line[30],\n",
    "#                                t1_ban2 =               line[31],\n",
    "#                                t1_ban3 =               line[32],\n",
    "#                                t1_ban4 =               line[33],\n",
    "#                                t1_ban5 =               line[34],\n",
    "#                                t2_champ1id =           line[35],\n",
    "#                                t2_champ1_sum1 =        line[36],\n",
    "#                                t2_champ1_sum2 =        line[37],\n",
    "#                                t2_champ2id =           line[38],\n",
    "#                                t2_champ2_sum1 =        line[39],\n",
    "#                                t2_champ2_sum2 =        line[40],\n",
    "#                                t2_champ3id =           line[41],\n",
    "#                                t2_champ3_sum1 =        line[42],\n",
    "#                                t2_champ3_sum2 =        line[43],\n",
    "#                                t2_champ4id =           line[44],\n",
    "#                                t2_champ4_sum1 =        line[45],\n",
    "#                                t2_champ4_sum2 =        line[46],\n",
    "#                                t2_champ5id =           line[47],\n",
    "#                                t2_champ5_sum1 =        line[48],\n",
    "#                                t2_champ5_sum2 =        line[49],\n",
    "#                                t2_towerKills =         line[50],\n",
    "#                                t2_inhibitorKills =     line[51],\n",
    "#                                t2_baronKills =         line[52],\n",
    "#                                t2_dragonKills =        line[53],\n",
    "#                                t2_riftHeraldKills =    line[54],\n",
    "#                                t2_ban1 =               line[55],\n",
    "#                                t2_ban2 =               line[56],\n",
    "#                                t2_ban3 =               line[57],\n",
    "#                                t2_ban4 =               line[58],\n",
    "#                                t2_ban5 =               line[59])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding rows that are not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "excludes = [\n",
    " 't1_ban1',\n",
    " 't1_ban2',\n",
    " 't1_ban3',\n",
    " 't1_ban4',\n",
    " 't1_ban5',\n",
    " 't1_baronKills',\n",
    " 't1_champ1_sum1',\n",
    " 't1_champ1_sum2',\n",
    " 't1_champ1id',\n",
    " 't1_champ2_sum1',\n",
    " 't1_champ2_sum2',\n",
    " 't1_champ2id',\n",
    " 't1_champ3_sum1',\n",
    " 't1_champ3_sum2',\n",
    " 't1_champ3id',\n",
    " 't1_champ4_sum1',\n",
    " 't1_champ4_sum2',\n",
    " 't1_champ4id',\n",
    " 't1_champ5_sum1',\n",
    " 't1_champ5_sum2',\n",
    " 't1_champ5id',\n",
    " 't2_ban1',\n",
    " 't2_ban2',\n",
    " 't2_ban3',\n",
    " 't2_ban4',\n",
    " 't2_ban5',\n",
    " 't2_baronKills',\n",
    " 't2_champ1_sum1',\n",
    " 't2_champ1_sum2',\n",
    " 't2_champ1id',\n",
    " 't2_champ2_sum1',\n",
    " 't2_champ2_sum2',\n",
    " 't2_champ2id',\n",
    " 't2_champ3_sum1',\n",
    " 't2_champ3_sum2',\n",
    " 't2_champ3id',\n",
    " 't2_champ4_sum1',\n",
    " 't2_champ4_sum2',\n",
    " 't2_champ4id',\n",
    " 't2_champ5_sum1',\n",
    " 't2_champ5_sum2',\n",
    " 't2_champ5id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for exclude in excludes:\n",
    "    df = df.drop(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gameId',\n",
       " 'gameDuration',\n",
       " 'seasonId',\n",
       " 'winner',\n",
       " 'firstBlood',\n",
       " 'firstTower',\n",
       " 'firstInhibitor',\n",
       " 'firstBaron',\n",
       " 'firstDragon',\n",
       " 'firstRiftHerald',\n",
       " 't1_towerKills',\n",
       " 't1_inhibitorKills',\n",
       " 't1_dragonKills',\n",
       " 't1_riftHeraldKills',\n",
       " 't2_towerKills',\n",
       " 't2_inhibitorKills',\n",
       " 't2_dragonKills',\n",
       " 't2_riftHeraldKills']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to define each row type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEPRECATED - AS We're importing it directly from CSV\n",
    "# Import all from `sql.types`\n",
    "#from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "#def convertColumn(df, names, newType):\n",
    "#  for name in names: \n",
    "#     df = df.withColumn(name, df[name].cast(newType))\n",
    "#  return df \n",
    "\n",
    "# Assign all column names to `columns`\n",
    "columns = ['firstBaron',\n",
    " 'firstBlood',\n",
    " 'firstDragon',\n",
    " 'firstInhibitor',\n",
    " 'firstRiftHerald',\n",
    " 'firstTower',\n",
    " 'gameDuration',\n",
    " 'gameId',\n",
    " 'seasonId',\n",
    " 't1_dragonKills',\n",
    " 't1_inhibitorKills',\n",
    " 't1_riftHeraldKills',\n",
    " 't1_towerKills',\n",
    " 't2_dragonKills',\n",
    " 't2_inhibitorKills',\n",
    " 't2_riftHeraldKills',\n",
    " 't2_towerKills',\n",
    " 'winner']\n",
    "\n",
    "# Conver the `df` columns to `FloatType()`\n",
    "#df = convertColumn(df, columns, IntegerType())\n",
    "\n",
    "#df = df.select(\"medianHouseValue\", \n",
    "#              \"totalBedRooms\", \n",
    "#              \"population\", \n",
    "#              \"households\", \n",
    "#              \"medianIncome\", \n",
    "#              \"roomsPerHousehold\", \n",
    "#              \"populationPerHousehold\", \n",
    "#              \"bedroomsPerRoom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gameId: long (nullable = true)\n",
      " |-- gameDuration: integer (nullable = true)\n",
      " |-- seasonId: integer (nullable = true)\n",
      " |-- winner: integer (nullable = true)\n",
      " |-- firstBlood: integer (nullable = true)\n",
      " |-- firstTower: integer (nullable = true)\n",
      " |-- firstInhibitor: integer (nullable = true)\n",
      " |-- firstBaron: integer (nullable = true)\n",
      " |-- firstDragon: integer (nullable = true)\n",
      " |-- firstRiftHerald: integer (nullable = true)\n",
      " |-- t1_towerKills: integer (nullable = true)\n",
      " |-- t1_inhibitorKills: integer (nullable = true)\n",
      " |-- t1_dragonKills: integer (nullable = true)\n",
      " |-- t1_riftHeraldKills: integer (nullable = true)\n",
      " |-- t2_towerKills: integer (nullable = true)\n",
      " |-- t2_inhibitorKills: integer (nullable = true)\n",
      " |-- t2_dragonKills: integer (nullable = true)\n",
      " |-- t2_riftHeraldKills: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gameId', 'bigint'),\n",
       " ('gameDuration', 'int'),\n",
       " ('seasonId', 'int'),\n",
       " ('winner', 'int'),\n",
       " ('firstBlood', 'int'),\n",
       " ('firstTower', 'int'),\n",
       " ('firstInhibitor', 'int'),\n",
       " ('firstBaron', 'int'),\n",
       " ('firstDragon', 'int'),\n",
       " ('firstRiftHerald', 'int'),\n",
       " ('t1_towerKills', 'int'),\n",
       " ('t1_inhibitorKills', 'int'),\n",
       " ('t1_dragonKills', 'int'),\n",
       " ('t1_riftHeraldKills', 'int'),\n",
       " ('t2_towerKills', 'int'),\n",
       " ('t2_inhibitorKills', 'int'),\n",
       " ('t2_dragonKills', 'int'),\n",
       " ('t2_riftHeraldKills', 'int')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+------+\n",
      "|t1_inhibitorKills|t2_towerKills|winner|\n",
      "+-----------------+-------------+------+\n",
      "|                1|            5|     0|\n",
      "|                4|            2|     0|\n",
      "|                1|            2|     0|\n",
      "|                2|            0|     0|\n",
      "|                2|            3|     0|\n",
      "|                1|            6|     0|\n",
      "|                2|            2|     0|\n",
      "|                0|            0|     0|\n",
      "|                0|            8|     1|\n",
      "|                1|            8|     1|\n",
      "|                0|           10|     1|\n",
      "|                1|            2|     0|\n",
      "|                0|            3|     0|\n",
      "|                1|            3|     0|\n",
      "|                1|            9|     1|\n",
      "+-----------------+-------------+------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('t1_inhibitorKills','t2_towerKills','winner').show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we need to start using MLlib - spark ml library\n",
    "\n",
    " - https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#logistic-regression\n",
    " - http://people.duke.edu/~ccc14/sta-663-2016/21D_Spark_MLib.html\n",
    " - https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
