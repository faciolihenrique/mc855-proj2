{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC855 - Data analysis pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing spark and data\n",
    " - Spark version 2.2.0\n",
    " - Change /home/henrique/Downloads/spark to the path you downloaded and extracted spark\n",
    " - You may need to export hadoop bin to your path before running jupyter notebook \n",
    "   - `export PATH=$PATH:/Users/hadoop/hadoop/bin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import findspark \n",
    "import findspark\n",
    "\n",
    "# Initialize and provide path\n",
    "findspark.init(\"/home/henrique/Downloads/spark\")\n",
    "\n",
    "# Or use this alternative\n",
    "#findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "       .master(\"local\") \\\n",
    "       .appName(\"Linear Regression Model\") \\\n",
    "       .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "       .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deprecated - Using direct load from CSV to df\n",
    "#file = sc.textFile(\"games.csv\")\n",
    "#header = file.first()                           #extract header\n",
    "#data = file.filter(lambda row: row != header)   #filter out header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to load directly from the csv\n",
    "if this is working, the last frame is not needed anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sql = pyspark.sql.SQLContext(sc)\n",
    "\n",
    "df = (sql.read\n",
    "         .format(\"com.databricks.spark.csv\")  # Choose the bib to oad csv\n",
    "         .option(\"header\", \"true\")            # Use the first line as header\n",
    "         .option(\"inferSchema\", \"true\")       # Try to infer data type - if this is not set all the typer will be str\n",
    "         .load(\"games.csv\"))                  # File name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[gameId: bigint, gameDuration: int, seasonId: int, winner: int, firstBlood: int, firstTower: int, firstInhibitor: int, firstBaron: int, firstDragon: int, firstRiftHerald: int, t1_champ1id: int, t1_champ1_sum1: int, t1_champ1_sum2: int, t1_champ2id: int, t1_champ2_sum1: int, t1_champ2_sum2: int, t1_champ3id: int, t1_champ3_sum1: int, t1_champ3_sum2: int, t1_champ4id: int, t1_champ4_sum1: int, t1_champ4_sum2: int, t1_champ5id: int, t1_champ5_sum1: int, t1_champ5_sum2: int, t1_towerKills: int, t1_inhibitorKills: int, t1_baronKills: int, t1_dragonKills: int, t1_riftHeraldKills: int, t1_ban1: int, t1_ban2: int, t1_ban3: int, t1_ban4: int, t1_ban5: int, t2_champ1id: int, t2_champ1_sum1: int, t2_champ1_sum2: int, t2_champ2id: int, t2_champ2_sum1: int, t2_champ2_sum2: int, t2_champ3id: int, t2_champ3_sum1: int, t2_champ3_sum2: int, t2_champ4id: int, t2_champ4_sum1: int, t2_champ4_sum2: int, t2_champ5id: int, t2_champ5_sum1: int, t2_champ5_sum2: int, t2_towerKills: int, t2_inhibitorKills: int, t2_baronKills: int, t2_dragonKills: int, t2_riftHeraldKills: int, t2_ban1: int, t2_ban2: int, t2_ban3: int, t2_ban4: int, t2_ban5: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just checking if data is Ok\n",
    "will print the first three lines of file and the last two of then must be equal to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#file.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting each element of the data list and turning each line in a list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#file = file.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the header of the rdd and turning it into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map the RDD to a DF\n",
    "#from pyspark.sql import Row\n",
    "\n",
    "#df = file.map(lambda line: Row(gameId =                 line[0],\n",
    "#                                gameDuration =          line[1],\n",
    "#                                seasonId =              line[2],\n",
    "#                                winner =                line[3],\n",
    "#                                firstBlood =            line[4],\n",
    "#                                firstTower =            line[5],\n",
    "#                                firstInhibitor =        line[6],\n",
    "#                                firstBaron =            line[7],\n",
    "#                                firstDragon =           line[8],\n",
    "#                                firstRiftHerald =       line[9],\n",
    "#                                t1_champ1id =           line[10],\n",
    "#                                t1_champ1_sum1 =        line[11],\n",
    "#                                t1_champ1_sum2 =        line[12],\n",
    "#                                t1_champ2id =           line[13],\n",
    "#                                t1_champ2_sum1 =        line[14],\n",
    "#                                t1_champ2_sum2 =        line[15],\n",
    "#                                t1_champ3id =           line[16],\n",
    "#                                t1_champ3_sum1 =        line[17],\n",
    "#                                t1_champ3_sum2 =        line[18],\n",
    "#                                t1_champ4id =           line[19],\n",
    "#                                t1_champ4_sum1 =        line[20],\n",
    "#                                t1_champ4_sum2 =        line[21],\n",
    "#                                t1_champ5id =           line[22],\n",
    "#                                t1_champ5_sum1 =        line[23],\n",
    "#                                t1_champ5_sum2 =        line[24],\n",
    "#                                t1_towerKills =         line[25],\n",
    "#                                t1_inhibitorKills =     line[26],\n",
    "#                                t1_baronKills =         line[27],\n",
    "#                                t1_dragonKills =        line[28],\n",
    "#                                t1_riftHeraldKills =    line[29],\n",
    "#                                t1_ban1 =               line[30],\n",
    "#                                t1_ban2 =               line[31],\n",
    "#                                t1_ban3 =               line[32],\n",
    "#                                t1_ban4 =               line[33],\n",
    "#                                t1_ban5 =               line[34],\n",
    "#                                t2_champ1id =           line[35],\n",
    "#                                t2_champ1_sum1 =        line[36],\n",
    "#                                t2_champ1_sum2 =        line[37],\n",
    "#                                t2_champ2id =           line[38],\n",
    "#                                t2_champ2_sum1 =        line[39],\n",
    "#                                t2_champ2_sum2 =        line[40],\n",
    "#                                t2_champ3id =           line[41],\n",
    "#                                t2_champ3_sum1 =        line[42],\n",
    "#                                t2_champ3_sum2 =        line[43],\n",
    "#                                t2_champ4id =           line[44],\n",
    "#                                t2_champ4_sum1 =        line[45],\n",
    "#                                t2_champ4_sum2 =        line[46],\n",
    "#                                t2_champ5id =           line[47],\n",
    "#                                t2_champ5_sum1 =        line[48],\n",
    "#                                t2_champ5_sum2 =        line[49],\n",
    "#                                t2_towerKills =         line[50],\n",
    "#                                t2_inhibitorKills =     line[51],\n",
    "#                                t2_baronKills =         line[52],\n",
    "#                                t2_dragonKills =        line[53],\n",
    "#                                t2_riftHeraldKills =    line[54],\n",
    "#                                t2_ban1 =               line[55],\n",
    "#                                t2_ban2 =               line[56],\n",
    "#                                t2_ban3 =               line[57],\n",
    "#                                t2_ban4 =               line[58],\n",
    "#                                t2_ban5 =               line[59])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding rows that are not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "excludes = [\n",
    " 't1_ban1',\n",
    " 't1_ban2',\n",
    " 't1_ban3',\n",
    " 't1_ban4',\n",
    " 't1_ban5',\n",
    " 't1_baronKills',\n",
    " 't1_champ1_sum1',\n",
    " 't1_champ1_sum2',\n",
    " 't1_champ1id',\n",
    " 't1_champ2_sum1',\n",
    " 't1_champ2_sum2',\n",
    " 't1_champ2id',\n",
    " 't1_champ3_sum1',\n",
    " 't1_champ3_sum2',\n",
    " 't1_champ3id',\n",
    " 't1_champ4_sum1',\n",
    " 't1_champ4_sum2',\n",
    " 't1_champ4id',\n",
    " 't1_champ5_sum1',\n",
    " 't1_champ5_sum2',\n",
    " 't1_champ5id',\n",
    " 't2_ban1',\n",
    " 't2_ban2',\n",
    " 't2_ban3',\n",
    " 't2_ban4',\n",
    " 't2_ban5',\n",
    " 't2_baronKills',\n",
    " 't2_champ1_sum1',\n",
    " 't2_champ1_sum2',\n",
    " 't2_champ1id',\n",
    " 't2_champ2_sum1',\n",
    " 't2_champ2_sum2',\n",
    " 't2_champ2id',\n",
    " 't2_champ3_sum1',\n",
    " 't2_champ3_sum2',\n",
    " 't2_champ3id',\n",
    " 't2_champ4_sum1',\n",
    " 't2_champ4_sum2',\n",
    " 't2_champ4id',\n",
    " 't2_champ5_sum1',\n",
    " 't2_champ5_sum2',\n",
    " 't2_champ5id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for exclude in excludes:\n",
    "    df = df.drop(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gameId',\n",
       " 'gameDuration',\n",
       " 'seasonId',\n",
       " 'winner',\n",
       " 'firstBlood',\n",
       " 'firstTower',\n",
       " 'firstInhibitor',\n",
       " 'firstBaron',\n",
       " 'firstDragon',\n",
       " 'firstRiftHerald',\n",
       " 't1_towerKills',\n",
       " 't1_inhibitorKills',\n",
       " 't1_dragonKills',\n",
       " 't1_riftHeraldKills',\n",
       " 't2_towerKills',\n",
       " 't2_inhibitorKills',\n",
       " 't2_dragonKills',\n",
       " 't2_riftHeraldKills']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to define each row type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEPRECATED - AS We're importing it directly from CSV\n",
    "# Import all from `sql.types`\n",
    "#from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "#def convertColumn(df, names, newType):\n",
    "#  for name in names: \n",
    "#     df = df.withColumn(name, df[name].cast(newType))\n",
    "#  return df \n",
    "\n",
    "# Assign all column names to `columns`\n",
    "columns = ['firstBaron',\n",
    " 'firstBlood',\n",
    " 'firstDragon',\n",
    " 'firstInhibitor',\n",
    " 'firstRiftHerald',\n",
    " 'firstTower',\n",
    " 'gameDuration',\n",
    " 'gameId',\n",
    " 'seasonId',\n",
    " 't1_dragonKills',\n",
    " 't1_inhibitorKills',\n",
    " 't1_riftHeraldKills',\n",
    " 't1_towerKills',\n",
    " 't2_dragonKills',\n",
    " 't2_inhibitorKills',\n",
    " 't2_riftHeraldKills',\n",
    " 't2_towerKills',\n",
    " 'winner']\n",
    "\n",
    "# Conver the `df` columns to `FloatType()`\n",
    "#df = convertColumn(df, columns, IntegerType())\n",
    "\n",
    "#df = df.select(\"medianHouseValue\", \n",
    "#              \"totalBedRooms\", \n",
    "#              \"population\", \n",
    "#              \"households\", \n",
    "#              \"medianIncome\", \n",
    "#              \"roomsPerHousehold\", \n",
    "#              \"populationPerHousehold\", \n",
    "#              \"bedroomsPerRoom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gameId: long (nullable = true)\n",
      " |-- gameDuration: integer (nullable = true)\n",
      " |-- seasonId: integer (nullable = true)\n",
      " |-- winner: integer (nullable = true)\n",
      " |-- firstBlood: integer (nullable = true)\n",
      " |-- firstTower: integer (nullable = true)\n",
      " |-- firstInhibitor: integer (nullable = true)\n",
      " |-- firstBaron: integer (nullable = true)\n",
      " |-- firstDragon: integer (nullable = true)\n",
      " |-- firstRiftHerald: integer (nullable = true)\n",
      " |-- t1_towerKills: integer (nullable = true)\n",
      " |-- t1_inhibitorKills: integer (nullable = true)\n",
      " |-- t1_dragonKills: integer (nullable = true)\n",
      " |-- t1_riftHeraldKills: integer (nullable = true)\n",
      " |-- t2_towerKills: integer (nullable = true)\n",
      " |-- t2_inhibitorKills: integer (nullable = true)\n",
      " |-- t2_dragonKills: integer (nullable = true)\n",
      " |-- t2_riftHeraldKills: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gameId', 'bigint'),\n",
       " ('gameDuration', 'int'),\n",
       " ('seasonId', 'int'),\n",
       " ('winner', 'int'),\n",
       " ('firstBlood', 'int'),\n",
       " ('firstTower', 'int'),\n",
       " ('firstInhibitor', 'int'),\n",
       " ('firstBaron', 'int'),\n",
       " ('firstDragon', 'int'),\n",
       " ('firstRiftHerald', 'int'),\n",
       " ('t1_towerKills', 'int'),\n",
       " ('t1_inhibitorKills', 'int'),\n",
       " ('t1_dragonKills', 'int'),\n",
       " ('t1_riftHeraldKills', 'int'),\n",
       " ('t2_towerKills', 'int'),\n",
       " ('t2_inhibitorKills', 'int'),\n",
       " ('t2_dragonKills', 'int'),\n",
       " ('t2_riftHeraldKills', 'int')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+------+\n",
      "|t1_inhibitorKills|t2_towerKills|winner|\n",
      "+-----------------+-------------+------+\n",
      "|                1|            5|     0|\n",
      "|                4|            2|     0|\n",
      "|                1|            2|     0|\n",
      "|                2|            0|     0|\n",
      "|                2|            3|     0|\n",
      "|                1|            6|     0|\n",
      "|                2|            2|     0|\n",
      "|                0|            0|     0|\n",
      "|                0|            8|     1|\n",
      "|                1|            8|     1|\n",
      "|                0|           10|     1|\n",
      "|                1|            2|     0|\n",
      "|                0|            3|     0|\n",
      "|                1|            3|     0|\n",
      "|                1|            9|     1|\n",
      "+-----------------+-------------+------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('t1_inhibitorKills','t2_towerKills','winner').show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we need to start using MLlib - spark ml library\n",
    "\n",
    " - https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#logistic-regression\n",
    " - http://people.duke.edu/~ccc14/sta-663-2016/21D_Spark_MLib.html\n",
    " - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = (VectorAssembler()\n",
    "    .setInputCols(df.columns[1:])\n",
    "    .setOutputCol(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"label\" does not exist.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/home/henrique/Downloads/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/henrique/Downloads/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o218.fit.\n: java.lang.IllegalArgumentException: Field \"label\" does not exist.\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:266)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:266)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:265)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:71)\n\tat org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:53)\n\tat org.apache.spark.ml.classification.Classifier.org$apache$spark$ml$classification$ClassifierParams$$super$validateAndTransformSchema(Classifier.scala:58)\n\tat org.apache.spark.ml.classification.ClassifierParams$class.validateAndTransformSchema(Classifier.scala:42)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifier.org$apache$spark$ml$classification$ProbabilisticClassifierParams$$super$validateAndTransformSchema(ProbabilisticClassifier.scala:53)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifierParams$class.validateAndTransformSchema(ProbabilisticClassifier.scala:37)\n\tat org.apache.spark.ml.classification.LogisticRegression.org$apache$spark$ml$classification$LogisticRegressionParams$$super$validateAndTransformSchema(LogisticRegression.scala:278)\n\tat org.apache.spark.ml.classification.LogisticRegressionParams$class.validateAndTransformSchema(LogisticRegression.scala:265)\n\tat org.apache.spark.ml.classification.LogisticRegression.validateAndTransformSchema(LogisticRegression.scala:278)\n\tat org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:144)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:100)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-0c9acad8dec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massembler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/henrique/Downloads/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/home/henrique/Downloads/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/henrique/Downloads/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/home/henrique/Downloads/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/henrique/Downloads/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/henrique/Downloads/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/henrique/Downloads/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"label\" does not exist.'"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
